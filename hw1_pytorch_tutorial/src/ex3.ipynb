{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9f180e",
   "metadata": {},
   "source": [
    "# Exercise 3: Neural networks in PyTorch\n",
    "\n",
    "In this exercise you’ll implement small neural-network building blocks from scratch and use them to train a simple classifier.\n",
    "\n",
    "You’ll cover:\n",
    "- **Basic layers**: Linear, Embedding, Dropout\n",
    "- **Normalization**: LayerNorm and RMSNorm\n",
    "- **MLPs + residual**: composing layers into deeper networks\n",
    "- **Classification**: generating a learnable dataset, implementing cross-entropy from logits, and writing a minimal training loop\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "Use small sanity checks and compare to PyTorch reference implementations when useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948aeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aaebe3",
   "metadata": {},
   "source": [
    "## Basic layers\n",
    "\n",
    "In this section you’ll implement a few core layers that appear everywhere:\n",
    "\n",
    "### `Linear`\n",
    "A fully-connected layer that follows nn.Linear conventions:  \n",
    "`y = x @ Wᵀ + b`\n",
    "\n",
    "Important details:\n",
    "- Parameters should be registered as `nn.Parameter`\n",
    "- Store weight as (out_features, in_features) like nn.Linear.\n",
    "- The forward pass should support leading batch dimensions: `x` can be shape `(..., in_features)`\n",
    "\n",
    "### `Embedding`\n",
    "An embedding table maps integer ids to vectors:\n",
    "- input: token ids `idx` of shape `(...,)`\n",
    "- output: vectors of shape `(..., embedding_dim)`\n",
    "\n",
    "This is essentially a learnable lookup table.\n",
    "\n",
    "### `Dropout`\n",
    "Dropout randomly zeroes activations during training to reduce overfitting.\n",
    "Implementation details:\n",
    "- Only active in `model.train()` mode\n",
    "- In training: drop with probability `p` and scale the kept values by `1/(1-p)` so the expected value stays the same\n",
    "- In eval: return the input unchanged\n",
    "\n",
    "## Instructions\n",
    "- Do not use PyTorch reference modules for the parts you implement (e.g. don’t call nn.Linear inside your Linear).\n",
    "- You may use standard tensor ops that you learned before (matmul, sum, mean, rsqrt, indexing, etc.).\n",
    "- Use a parameter initialization method of your choice. We recommend something like Xavier-uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa9b6f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1575, -0.6848,  0.0579,  0.2306]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        weight = torch.zeros(size=(out_features, in_features))\n",
    "        torch.nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros([out_features]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., in_features)\n",
    "        return: (..., out_features)\n",
    "        \"\"\"\n",
    "        result = x@self.weight.T\n",
    "        if self.bias is not None:\n",
    "            result += self.bias\n",
    "        return result\n",
    "        \n",
    "Linear(5,4).forward(torch.ones(1,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2241e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4198,  0.5900,  0.0508]],\n",
       "\n",
       "         [[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4743,  0.0624,  0.2419]]],\n",
       "\n",
       "\n",
       "        [[[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4198,  0.5900,  0.0508]],\n",
       "\n",
       "         [[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4743,  0.0624,  0.2419]]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(num_embeddings, embedding_dim))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (...,) int64\n",
    "        return: (..., embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.weight[idx]\n",
    "\n",
    "Embedding(10,3).forward(torch.tensor([[[0,1,1],[0,1,2]],[[0,1,1],[0,1,2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3390686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.4286, 1.4286, 1.4286, 1.4286, 0.0000],\n",
       "         [0.0000, 1.4286, 1.4286, 1.4286, 0.0000],\n",
       "         [0.0000, 1.4286, 0.0000, 1.4286, 0.0000],\n",
       "         [1.4286, 1.4286, 1.4286, 0.0000, 1.4286],\n",
       "         [1.4286, 1.4286, 1.4286, 1.4286, 1.4286]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        In train mode: drop with prob p and scale by 1/(1-p).\n",
    "        In eval mode: return x unchanged.\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return x\n",
    "        prob = torch.zeros_like(x)\n",
    "        prob.uniform_(0,1)\n",
    "        return x.where(prob > self.p, 0) * (1/(1-self.p))\n",
    "\n",
    "dropout = Dropout(0.3)\n",
    "dropout.train()\n",
    "dropout.forward(torch.ones(1,5,5))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef77371",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization layers help stabilize training by controlling activation statistics.\n",
    "\n",
    "### LayerNorm\n",
    "LayerNorm normalizes each example across its **feature dimension** (the last dimension):\n",
    "\n",
    "- compute mean and variance over the last dimension\n",
    "- normalize: `(x - mean) / sqrt(var + eps)`\n",
    "- apply learnable per-feature scale and shift (`weight`, `bias`)\n",
    "\n",
    "**In this exercise, assume `elementwise_affine=True` (always include `weight` and `bias`).**  \n",
    "`weight` and `bias` each have shape `(D,)`.\n",
    "\n",
    "LayerNorm is widely used in transformers because it does not depend on batch statistics.\n",
    "\n",
    "### RMSNorm\n",
    "RMSNorm is similar to LayerNorm but normalizes using only the root-mean-square:\n",
    "- `x / sqrt(mean(x^2) + eps)` over the last dimension\n",
    "- usually includes a learnable scale (`weight`)\n",
    "- no mean subtraction\n",
    "\n",
    "RMSNorm is popular in modern LLMs because it's faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aeaceef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0989, -0.3268,  0.2099,  0.3278,  0.3323, -0.3406,  0.1972,  0.1293,\n",
      "        -0.1042,  0.1071])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1504e-01, -1.2176e+00,  2.5555e+00, -3.8171e-01, -3.2971e+00,\n",
       "         -5.6900e-01,  6.3473e-01, -1.0426e+00,  1.2167e+00, -2.3417e-03],\n",
       "        [ 5.1640e-01, -1.5591e+00, -2.6643e-01, -7.4787e-01, -1.0611e+00,\n",
       "          2.2523e-01, -8.6268e-01, -2.1266e-01,  1.2820e-01, -1.3549e+00],\n",
       "        [-1.0260e+00, -3.1809e-01, -2.5542e-01,  7.1209e-01,  2.0921e+00,\n",
       "          1.0925e-01, -8.1726e-02, -1.1531e-01,  7.7773e-01,  3.4758e-01],\n",
       "        [ 9.4283e-01,  2.5441e+00,  1.3201e+00, -1.5489e+00, -2.0292e+00,\n",
       "          1.7472e+00, -1.2504e+00, -1.2474e-01,  1.2491e-01,  2.1027e+00],\n",
       "        [ 3.3295e-01,  3.7805e+00,  3.7085e-01,  1.1326e-02, -7.7049e-01,\n",
       "          3.9304e-01, -3.1336e-01, -1.3047e+00,  9.5967e-01,  8.4602e-01],\n",
       "        [-8.3052e-02, -4.4530e+00,  4.5396e-01,  6.6868e-01, -1.0176e+00,\n",
       "         -1.0889e+00,  6.5812e-01,  7.8234e-02, -6.7013e-01, -9.0464e-01],\n",
       "        [-6.1597e-01, -7.6302e-01, -1.9380e-01,  1.4401e-01, -1.6850e+00,\n",
       "          1.2994e+00,  1.8872e+00, -7.1271e-01,  8.1449e-01,  4.5957e-01],\n",
       "        [-9.1935e-01,  2.1222e-01,  4.1824e-01, -1.5836e-01, -3.6329e-01,\n",
       "          1.6127e+00, -6.5725e-01, -6.3131e-01,  1.4262e+00, -2.2060e-01],\n",
       "        [ 2.4346e-01,  1.8320e+00,  4.2329e-02, -1.2795e+00, -1.7721e+00,\n",
       "         -3.5870e-01, -1.3323e+00,  1.5905e+00,  1.5125e+00, -1.5835e+00],\n",
       "        [ 6.1813e-01,  6.0801e-01, -2.5450e-01, -3.5869e-01,  4.5033e-01,\n",
       "          1.5031e+00, -9.7762e-01, -1.4034e+00,  8.7537e-01, -8.3021e-01]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, normalized_shape: int, eps: float = 1e-5, elementwise_affine: bool = True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.weight = nn.Parameter(torch.ones([normalized_shape]))\n",
    "        self.bias = nn.Parameter(torch.zeros([normalized_shape]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize over the last dimension.\n",
    "        x: (..., D)\n",
    "        \"\"\"\n",
    "        layer_mean = torch.mean(x, dim=-1)\n",
    "        layer_std = torch.std(x, dim=-1, correction=0)\n",
    "        print(layer_mean)\n",
    "        result = (x - layer_mean) / (layer_std + self.eps)\n",
    "        return result*self.weight + self.bias\n",
    "        \n",
    "LayerNorm(10).forward(torch.randn(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08a2ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2734, -1.1069,  0.0739,  0.2700,  1.6816,  0.3672, -0.5372,  0.8667,\n",
       "         -1.7350, -0.2491]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.weight = nn.Parameter(torch.ones([normalized_shape]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RMSNorm: x / sqrt(mean(x^2) + eps) * weight\n",
    "        over the last dimension.\n",
    "        \"\"\"\n",
    "        return (x / (torch.mean(x**2, dim=-1, keepdim=True)**0.5 + self.eps)) * self.weight\n",
    "RMSNorm(10).forward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2b352",
   "metadata": {},
   "source": [
    "## MLPs and residual networks\n",
    "\n",
    "Now you’ll build larger networks by composing layers.\n",
    "\n",
    "### MLP\n",
    "An MLP is a stack of `depth` Linear layers with non-linear activations (use GELU) in between.\n",
    "In this exercise you’ll support:\n",
    "- configurable depth\n",
    "- a hidden dimension\n",
    "- optional LayerNorm between layers (a common stabilization trick)\n",
    "\n",
    "A key skill is building networks using `nn.ModuleList` / `nn.Sequential` while keeping shapes consistent.\n",
    "\n",
    "### Transformer-style FeedForward (FFN)\n",
    "A transformer block contains a position-wise feedforward network:\n",
    "- `D -> 4D -> D` (by default)\n",
    "- activation is typically **GELU**\n",
    "\n",
    "This is essentially an MLP applied independently at each token position.\n",
    "\n",
    "### Residual wrapper\n",
    "Residual connections are the simplest form of “skip connection”:\n",
    "- output is `x + fn(x)`\n",
    "\n",
    "They improve gradient flow and allow training deeper networks more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22436c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int,\n",
    "        use_layernorm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: build modules (list of Linear + activation)\n",
    "        # Optionally insert LayerNorm between layers.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2169774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-style FFN: D -> 4D -> D (default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        # TODO: create two Linear layers and choose an activation (GELU)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eef3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn: nn.Module):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        # TODO: return x + fn(x, ...)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b19d1a",
   "metadata": {},
   "source": [
    "## Classification problem\n",
    "\n",
    "In this section you’ll put everything together in a minimal MNIST classification experiment.\n",
    "\n",
    "You will:\n",
    "1) download and load the MNIST dataset\n",
    "2) implement cross-entropy from logits (stable, using log-softmax)\n",
    "3) build a simple MLP-based classifier (flatten MNIST images first)\n",
    "4) write a minimal training loop\n",
    "5) report train loss curve and final accuracy\n",
    "\n",
    "The goal here is not to reach state-of-the-art accuracy, but to understand the full pipeline:\n",
    "data → model → logits → loss → gradients → parameter update.\n",
    "\n",
    "### Model notes\n",
    "- We want you to combine the MLP we implemented above with the classification head we define below into one model \n",
    "\n",
    "### MNIST notes\n",
    "- MNIST images are `28×28` grayscale.\n",
    "- After `ToTensor()`, each image has shape `(1, 28, 28)` and values in `[0, 1]`.\n",
    "- For an MLP classifier, we flatten to a vector of length `784`.\n",
    "\n",
    "## Deliverables\n",
    "- Include a plot of your train loss curve in the video submission as well as a final accuracy. \n",
    "- **NOTE** Here we don't grade on model performance but we expect you to achieve at least 70% accuracy to confirm a correct model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c05009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()  # -> float32 in [0,1], shape (1, 28, 28)\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# TODO: define the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3781450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute mean cross-entropy loss from logits.\n",
    "\n",
    "    logits: (B, C)\n",
    "    targets: (B,) int64\n",
    "\n",
    "    Requirements:\n",
    "    - Use log-softmax for stability (do not use torch.nn.CrossEntropyLoss, we check this in the autograder).\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71242e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_in: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., d_in)\n",
    "        return: (..., num_classes) logits\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    # TODO: You can use this function to evaluate your model accuracy.\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20555e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "    model: nn.Module,\n",
    "    train_data_loader: DataLoader,\n",
    "    test_data_loader: DataLoader,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    seed: int = 0,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Minimal training loop for MNIST classification.\n",
    "\n",
    "    Steps:\n",
    "    - define optimizer\n",
    "    - for each epoch:\n",
    "        - sample minibatches\n",
    "        - forward -> cross-entropy -> backward -> optimizer step\n",
    "      - compute test accuracy at the end of each epoch\n",
    "    - return list of training losses (one per update step)\n",
    "\n",
    "    Requirements:\n",
    "    - call model.train() during training and model.eval() during evaluation\n",
    "    - do not use torch.nn.CrossEntropyLoss (use your cross_entropy_from_logits)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdad3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
