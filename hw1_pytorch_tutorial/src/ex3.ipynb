{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9f180e",
   "metadata": {},
   "source": [
    "# Exercise 3: Neural networks in PyTorch\n",
    "\n",
    "In this exercise you’ll implement small neural-network building blocks from scratch and use them to train a simple classifier.\n",
    "\n",
    "You’ll cover:\n",
    "- **Basic layers**: Linear, Embedding, Dropout\n",
    "- **Normalization**: LayerNorm and RMSNorm\n",
    "- **MLPs + residual**: composing layers into deeper networks\n",
    "- **Classification**: generating a learnable dataset, implementing cross-entropy from logits, and writing a minimal training loop\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "Use small sanity checks and compare to PyTorch reference implementations when useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948aeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aaebe3",
   "metadata": {},
   "source": [
    "## Basic layers\n",
    "\n",
    "In this section you’ll implement a few core layers that appear everywhere:\n",
    "\n",
    "### `Linear`\n",
    "A fully-connected layer that follows nn.Linear conventions:  \n",
    "`y = x @ Wᵀ + b`\n",
    "\n",
    "Important details:\n",
    "- Parameters should be registered as `nn.Parameter`\n",
    "- Store weight as (out_features, in_features) like nn.Linear.\n",
    "- The forward pass should support leading batch dimensions: `x` can be shape `(..., in_features)`\n",
    "\n",
    "### `Embedding`\n",
    "An embedding table maps integer ids to vectors:\n",
    "- input: token ids `idx` of shape `(...,)`\n",
    "- output: vectors of shape `(..., embedding_dim)`\n",
    "\n",
    "This is essentially a learnable lookup table.\n",
    "\n",
    "### `Dropout`\n",
    "Dropout randomly zeroes activations during training to reduce overfitting.\n",
    "Implementation details:\n",
    "- Only active in `model.train()` mode\n",
    "- In training: drop with probability `p` and scale the kept values by `1/(1-p)` so the expected value stays the same\n",
    "- In eval: return the input unchanged\n",
    "\n",
    "## Instructions\n",
    "- Do not use PyTorch reference modules for the parts you implement (e.g. don’t call nn.Linear inside your Linear).\n",
    "- You may use standard tensor ops that you learned before (matmul, sum, mean, rsqrt, indexing, etc.).\n",
    "- Use a parameter initialization method of your choice. We recommend something like Xavier-uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # TODO: initialize parameters\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., in_features)\n",
    "        return: (..., out_features)\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2241e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        # TODO: initialize\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (...,) int64\n",
    "        return: (..., embedding_dim)\n",
    "        \"\"\"\n",
    "        # TODO: implement (index into weight)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        In train mode: drop with prob p and scale by 1/(1-p).\n",
    "        In eval mode: return x unchanged.\n",
    "        \"\"\"\n",
    "        # TODO: implement without using nn.Dropout\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef77371",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization layers help stabilize training by controlling activation statistics.\n",
    "\n",
    "### LayerNorm\n",
    "LayerNorm normalizes each example across its **feature dimension** (the last dimension):\n",
    "\n",
    "- compute mean and variance over the last dimension\n",
    "- normalize: `(x - mean) / sqrt(var + eps)`\n",
    "- apply learnable per-feature scale and shift (`weight`, `bias`)\n",
    "\n",
    "**In this exercise, assume `elementwise_affine=True` (always include `weight` and `bias`).**  \n",
    "`weight` and `bias` each have shape `(D,)`.\n",
    "\n",
    "LayerNorm is widely used in transformers because it does not depend on batch statistics.\n",
    "\n",
    "### RMSNorm\n",
    "RMSNorm is similar to LayerNorm but normalizes using only the root-mean-square:\n",
    "- `x / sqrt(mean(x^2) + eps)` over the last dimension\n",
    "- usually includes a learnable scale (`weight`)\n",
    "- no mean subtraction\n",
    "\n",
    "RMSNorm is popular in modern LLMs because it's faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaceef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, normalized_shape: int, eps: float = 1e-5, elementwise_affine: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize over the last dimension.\n",
    "        x: (..., D)\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RMSNorm: x / sqrt(mean(x^2) + eps) * weight\n",
    "        over the last dimension.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2b352",
   "metadata": {},
   "source": [
    "## MLPs and residual networks\n",
    "\n",
    "Now you’ll build larger networks by composing layers.\n",
    "\n",
    "### MLP\n",
    "An MLP is a stack of `depth` Linear layers with non-linear activations (use GELU) in between.\n",
    "In this exercise you’ll support:\n",
    "- configurable depth\n",
    "- a hidden dimension\n",
    "- optional LayerNorm between layers (a common stabilization trick)\n",
    "\n",
    "A key skill is building networks using `nn.ModuleList` / `nn.Sequential` while keeping shapes consistent.\n",
    "\n",
    "### Transformer-style FeedForward (FFN)\n",
    "A transformer block contains a position-wise feedforward network:\n",
    "- `D -> 4D -> D` (by default)\n",
    "- activation is typically **GELU**\n",
    "\n",
    "This is essentially an MLP applied independently at each token position.\n",
    "\n",
    "### Residual wrapper\n",
    "Residual connections are the simplest form of “skip connection”:\n",
    "- output is `x + fn(x)`\n",
    "\n",
    "They improve gradient flow and allow training deeper networks more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22436c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int,\n",
    "        use_layernorm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: build modules (list of Linear + activation)\n",
    "        # Optionally insert LayerNorm between layers.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2169774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-style FFN: D -> 4D -> D (default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        # TODO: create two Linear layers and choose an activation (GELU)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eef3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn: nn.Module):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        # TODO: return x + fn(x, ...)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b19d1a",
   "metadata": {},
   "source": [
    "## Classification problem\n",
    "\n",
    "In this section you’ll put everything together in a minimal MNIST classification experiment.\n",
    "\n",
    "You will:\n",
    "1) download and load the MNIST dataset\n",
    "2) implement cross-entropy from logits (stable, using log-softmax)\n",
    "3) build a simple MLP-based classifier (flatten MNIST images first)\n",
    "4) write a minimal training loop\n",
    "5) report train loss curve and final accuracy\n",
    "\n",
    "The goal here is not to reach state-of-the-art accuracy, but to understand the full pipeline:\n",
    "data → model → logits → loss → gradients → parameter update.\n",
    "\n",
    "### Model notes\n",
    "- We want you to combine the MLP we implemented above with the classification head we define below into one model \n",
    "\n",
    "### MNIST notes\n",
    "- MNIST images are `28×28` grayscale.\n",
    "- After `ToTensor()`, each image has shape `(1, 28, 28)` and values in `[0, 1]`.\n",
    "- For an MLP classifier, we flatten to a vector of length `784`.\n",
    "\n",
    "## Deliverables\n",
    "- Include a plot of your train loss curve in the video submission as well as a final accuracy. \n",
    "- **NOTE** Here we don't grade on model performance but we expect you to achieve at least 70% accuracy to confirm a correct model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c05009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()  # -> float32 in [0,1], shape (1, 28, 28)\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# TODO: define the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3781450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute mean cross-entropy loss from logits.\n",
    "\n",
    "    logits: (B, C)\n",
    "    targets: (B,) int64\n",
    "\n",
    "    Requirements:\n",
    "    - Use log-softmax for stability (do not use torch.nn.CrossEntropyLoss, we check this in the autograder).\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71242e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_in: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., d_in)\n",
    "        return: (..., num_classes) logits\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    # TODO: You can use this function to evaluate your model accuracy.\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20555e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "    model: nn.Module,\n",
    "    train_data_loader: DataLoader,\n",
    "    test_data_loader: DataLoader,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    seed: int = 0,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Minimal training loop for MNIST classification.\n",
    "\n",
    "    Steps:\n",
    "    - define optimizer\n",
    "    - for each epoch:\n",
    "        - sample minibatches\n",
    "        - forward -> cross-entropy -> backward -> optimizer step\n",
    "      - compute test accuracy at the end of each epoch\n",
    "    - return list of training losses (one per update step)\n",
    "\n",
    "    Requirements:\n",
    "    - call model.train() during training and model.eval() during evaluation\n",
    "    - do not use torch.nn.CrossEntropyLoss (use your cross_entropy_from_logits)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdad3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
