{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9f180e",
   "metadata": {},
   "source": [
    "# Exercise 3: Neural networks in PyTorch\n",
    "\n",
    "In this exercise you’ll implement small neural-network building blocks from scratch and use them to train a simple classifier.\n",
    "\n",
    "You’ll cover:\n",
    "- **Basic layers**: Linear, Embedding, Dropout\n",
    "- **Normalization**: LayerNorm and RMSNorm\n",
    "- **MLPs + residual**: composing layers into deeper networks\n",
    "- **Classification**: generating a learnable dataset, implementing cross-entropy from logits, and writing a minimal training loop\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "Use small sanity checks and compare to PyTorch reference implementations when useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948aeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aaebe3",
   "metadata": {},
   "source": [
    "## Basic layers\n",
    "\n",
    "In this section you’ll implement a few core layers that appear everywhere:\n",
    "\n",
    "### `Linear`\n",
    "A fully-connected layer that follows nn.Linear conventions:  \n",
    "`y = x @ Wᵀ + b`\n",
    "\n",
    "Important details:\n",
    "- Parameters should be registered as `nn.Parameter`\n",
    "- Store weight as (out_features, in_features) like nn.Linear.\n",
    "- The forward pass should support leading batch dimensions: `x` can be shape `(..., in_features)`\n",
    "\n",
    "### `Embedding`\n",
    "An embedding table maps integer ids to vectors:\n",
    "- input: token ids `idx` of shape `(...,)`\n",
    "- output: vectors of shape `(..., embedding_dim)`\n",
    "\n",
    "This is essentially a learnable lookup table.\n",
    "\n",
    "### `Dropout`\n",
    "Dropout randomly zeroes activations during training to reduce overfitting.\n",
    "Implementation details:\n",
    "- Only active in `model.train()` mode\n",
    "- In training: drop with probability `p` and scale the kept values by `1/(1-p)` so the expected value stays the same\n",
    "- In eval: return the input unchanged\n",
    "\n",
    "## Instructions\n",
    "- Do not use PyTorch reference seq for the parts you implement (e.g. don’t call nn.Linear inside your Linear).\n",
    "- You may use standard tensor ops that you learned before (matmul, sum, mean, rsqrt, indexing, etc.).\n",
    "- Use a parameter initialization method of your choice. We recommend something like Xavier-uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa9b6f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1575, -0.6848,  0.0579,  0.2306]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        weight = torch.zeros(size=(out_features, in_features))\n",
    "        torch.nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros([out_features]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., in_features)\n",
    "        return: (..., out_features)\n",
    "        \"\"\"\n",
    "        result = x@self.weight.T\n",
    "        if self.bias is not None:\n",
    "            result += self.bias\n",
    "        return result\n",
    "        \n",
    "Linear(5,4).forward(torch.ones(1,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2241e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4198,  0.5900,  0.0508]],\n",
       "\n",
       "         [[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4743,  0.0624,  0.2419]]],\n",
       "\n",
       "\n",
       "        [[[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4198,  0.5900,  0.0508]],\n",
       "\n",
       "         [[-0.5734,  0.0258, -0.3688],\n",
       "          [ 0.4198,  0.5900,  0.0508],\n",
       "          [ 0.4743,  0.0624,  0.2419]]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(num_embeddings, embedding_dim))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (...,) int64\n",
    "        return: (..., embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.weight[idx]\n",
    "\n",
    "Embedding(10,3).forward(torch.tensor([[[0,1,1],[0,1,2]],[[0,1,1],[0,1,2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3390686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.4286, 1.4286, 1.4286, 1.4286, 0.0000],\n",
       "         [0.0000, 1.4286, 1.4286, 1.4286, 0.0000],\n",
       "         [0.0000, 1.4286, 0.0000, 1.4286, 0.0000],\n",
       "         [1.4286, 1.4286, 1.4286, 0.0000, 1.4286],\n",
       "         [1.4286, 1.4286, 1.4286, 1.4286, 1.4286]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        In train mode: drop with prob p and scale by 1/(1-p).\n",
    "        In eval mode: return x unchanged.\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return x\n",
    "        prob = torch.zeros_like(x)\n",
    "        prob.uniform_(0,1)\n",
    "        return x.where(prob > self.p, 0) * (1/(1-self.p))\n",
    "\n",
    "dropout = Dropout(0.3)\n",
    "dropout.train()\n",
    "dropout.forward(torch.ones(1,5,5))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef77371",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization layers help stabilize training by controlling activation statistics.\n",
    "\n",
    "### LayerNorm\n",
    "LayerNorm normalizes each example across its **feature dimension** (the last dimension):\n",
    "\n",
    "- compute mean and variance over the last dimension\n",
    "- normalize: `(x - mean) / sqrt(var + eps)`\n",
    "- apply learnable per-feature scale and shift (`weight`, `bias`)\n",
    "\n",
    "**In this exercise, assume `elementwise_affine=True` (always include `weight` and `bias`).**  \n",
    "`weight` and `bias` each have shape `(D,)`.\n",
    "\n",
    "LayerNorm is widely used in transformers because it does not depend on batch statistics.\n",
    "\n",
    "### RMSNorm\n",
    "RMSNorm is similar to LayerNorm but normalizes using only the root-mean-square:\n",
    "- `x / sqrt(mean(x^2) + eps)` over the last dimension\n",
    "- usually includes a learnable scale (`weight`)\n",
    "- no mean subtraction\n",
    "\n",
    "RMSNorm is popular in modern LLMs because it's faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aeaceef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5308],\n",
      "        [-0.2476],\n",
      "        [-0.0780],\n",
      "        [ 0.2179],\n",
      "        [-0.2628],\n",
      "        [ 0.8175],\n",
      "        [-0.3036],\n",
      "        [-0.3739],\n",
      "        [ 0.0928],\n",
      "        [-0.0058]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7212, -0.2500,  0.5137, -0.8735, -0.0339,  1.5019, -0.9226, -0.4309,\n",
       "         -1.5884,  0.3624],\n",
       "        [ 0.5822, -0.2835,  1.5853,  0.2786,  0.1764, -0.9101, -2.3438,  0.0599,\n",
       "          0.7935,  0.0614],\n",
       "        [ 0.3346,  2.3948,  0.4744, -0.2783, -0.7207, -0.1727, -0.3055, -0.4739,\n",
       "         -1.6748,  0.4222],\n",
       "        [ 0.2490, -1.6438, -0.7745,  0.8855, -0.6152, -0.7113,  1.9506, -0.4919,\n",
       "          0.2168,  0.9348],\n",
       "        [-0.0990,  0.2089, -1.3441,  0.6106,  0.5266, -2.3818,  0.7117,  0.3518,\n",
       "          0.4034,  1.0117],\n",
       "        [-1.0870,  1.1527,  0.9725, -1.4299,  1.4862, -0.4131,  0.2983, -1.3794,\n",
       "          0.3548,  0.0449],\n",
       "        [ 1.1003, -0.2366,  0.5563,  1.3115, -0.2625, -0.7898, -1.7349,  0.9315,\n",
       "          0.4975, -1.3733],\n",
       "        [-0.7397,  1.4580,  1.3117, -1.2939, -1.2074, -0.3741, -0.2025, -0.6473,\n",
       "          1.3154,  0.3797],\n",
       "        [-0.4675,  0.4941, -0.0987, -0.4186, -0.4090, -1.3625, -0.2001, -0.2910,\n",
       "          2.6831,  0.0702],\n",
       "        [ 0.7174, -0.0606,  0.0672, -0.9583,  0.2616, -0.1358,  1.1401, -1.6855,\n",
       "         -1.1078,  1.7617]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, normalized_shape: int, eps: float = 1e-5, elementwise_affine: bool = True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.weight = nn.Parameter(torch.ones([normalized_shape]))\n",
    "        self.bias = nn.Parameter(torch.zeros([normalized_shape]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize over the last dimension.\n",
    "        x: (..., D)\n",
    "        \"\"\"\n",
    "        layer_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        layer_var = torch.var(x, dim=-1, correction=0, keepdim=True)\n",
    "        print(layer_mean)\n",
    "        result = (x - layer_mean) / (layer_var + self.eps)**0.5\n",
    "        return result*self.weight + self.bias\n",
    "        \n",
    "LayerNorm(10).forward(torch.randn(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08a2ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2734, -1.1069,  0.0739,  0.2700,  1.6816,  0.3672, -0.5372,  0.8667,\n",
       "         -1.7350, -0.2491]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.weight = nn.Parameter(torch.ones([normalized_shape]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RMSNorm: x / sqrt(mean(x^2) + eps) * weight\n",
    "        over the last dimension.\n",
    "        \"\"\"\n",
    "        return (x / (torch.mean(x**2, dim=-1, keepdim=True)**0.5 + self.eps)) * self.weight\n",
    "RMSNorm(10).forward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2b352",
   "metadata": {},
   "source": [
    "## MLPs and residual networks\n",
    "\n",
    "Now you’ll build larger networks by composing layers.\n",
    "\n",
    "### MLP\n",
    "An MLP is a stack of `depth` Linear layers with non-linear activations (use GELU) in between.\n",
    "In this exercise you’ll support:\n",
    "- configurable depth\n",
    "- a hidden dimension\n",
    "- optional LayerNorm between layers (a common stabilization trick)\n",
    "\n",
    "A key skill is building networks using `nn.ModuleList` / `nn.Sequential` while keeping shapes consistent.\n",
    "\n",
    "### Transformer-style FeedForward (FFN)\n",
    "A transformer block contains a position-wise feedforward network:\n",
    "- `D -> 4D -> D` (by default)\n",
    "- activation is typically **GELU**\n",
    "\n",
    "This is essentially an MLP applied independently at each token position.\n",
    "\n",
    "### Residual wrapper\n",
    "Residual connections are the simplest form of “skip connection”:\n",
    "- output is `x + fn(x)`\n",
    "\n",
    "They improve gradient flow and allow training deeper networks more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22436c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (seq): Sequential(\n",
       "    (0): Linear()\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear()\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int,\n",
    "        use_layernorm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(Linear(in_dim, hidden_dim))\n",
    "        self.seq.append(nn.GELU())\n",
    "        for _ in range(depth):\n",
    "            if use_layernorm:\n",
    "                self.seq.append(LayerNorm(hidden_dim))\n",
    "            self.seq.append(Linear(hidden_dim, hidden_dim))\n",
    "            self.seq.append(nn.GELU())\n",
    "        if use_layernorm:\n",
    "            self.seq.append(LayerNorm(hidden_dim))\n",
    "        self.seq.append(Linear(hidden_dim, out_dim))        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.seq(x)\n",
    "\n",
    "MLP(10,10,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2169774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-style FFN: D -> 4D -> D (default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        # TODO: create two Linear layers and choose an activation (GELU)\n",
    "        self.linear1 = Linear(d_model, d_ff)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.gelu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eef3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn: nn.Module):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        return x + self.fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b19d1a",
   "metadata": {},
   "source": [
    "## Classification problem\n",
    "\n",
    "In this section you’ll put everything together in a minimal MNIST classification experiment.\n",
    "\n",
    "You will:\n",
    "1) download and load the MNIST dataset\n",
    "2) implement cross-entropy from logits (stable, using log-softmax)\n",
    "3) build a simple MLP-based classifier (flatten MNIST images first)\n",
    "4) write a minimal training loop\n",
    "5) report train loss curve and final accuracy\n",
    "\n",
    "The goal here is not to reach state-of-the-art accuracy, but to understand the full pipeline:\n",
    "data → model → logits → loss → gradients → parameter update.\n",
    "\n",
    "### Model notes\n",
    "- We want you to combine the MLP we implemented above with the classification head we define below into one model \n",
    "\n",
    "### MNIST notes\n",
    "- MNIST images are `28×28` grayscale.\n",
    "- After `ToTensor()`, each image has shape `(1, 28, 28)` and values in `[0, 1]`.\n",
    "- For an MLP classifier, we flatten to a vector of length `784`.\n",
    "\n",
    "## Deliverables\n",
    "- Include a plot of your train loss curve in the video submission as well as a final accuracy. \n",
    "- **NOTE** Here we don't grade on model performance but we expect you to achieve at least 70% accuracy to confirm a correct model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c05009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6306a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()  # -> float32 in [0,1], shape (1, 28, 28)\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# TODO: define the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3781450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute mean cross-entropy loss from logits.\n",
    "\n",
    "    logits: (B, C)\n",
    "    targets: (B,) int64\n",
    "\n",
    "    Requirements:\n",
    "    - Use log-softmax for stability (do not use torch.nn.CrossEntropyLoss, we check this in the autograder).\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71242e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_in: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., d_in)\n",
    "        return: (..., num_classes) logits\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    # TODO: You can use this function to evaluate your model accuracy.\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20555e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(\n",
    "    model: nn.Module,\n",
    "    train_data_loader: DataLoader,\n",
    "    test_data_loader: DataLoader,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    seed: int = 0,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Minimal training loop for MNIST classification.\n",
    "\n",
    "    Steps:\n",
    "    - define optimizer\n",
    "    - for each epoch:\n",
    "        - sample minibatches\n",
    "        - forward -> cross-entropy -> backward -> optimizer step\n",
    "      - compute test accuracy at the end of each epoch\n",
    "    - return list of training losses (one per update step)\n",
    "\n",
    "    Requirements:\n",
    "    - call model.train() during training and model.eval() during evaluation\n",
    "    - do not use torch.nn.CrossEntropyLoss (use your cross_entropy_from_logits)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdad3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
