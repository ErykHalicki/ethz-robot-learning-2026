{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9f180e",
   "metadata": {},
   "source": [
    "# Exercise 3: Neural networks in PyTorch\n",
    "\n",
    "In this exercise you’ll implement small neural-network building blocks from scratch and use them to train a simple classifier.\n",
    "\n",
    "You’ll cover:\n",
    "- **Basic layers**: Linear, Embedding, Dropout\n",
    "- **Normalization**: LayerNorm and RMSNorm\n",
    "- **MLPs + residual**: composing layers into deeper networks\n",
    "- **Classification**: generating a learnable dataset, implementing cross-entropy from logits, and writing a minimal training loop\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "Use small sanity checks and compare to PyTorch reference implementations when useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948aeb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aaebe3",
   "metadata": {},
   "source": [
    "## Basic layers\n",
    "\n",
    "In this section you’ll implement a few core layers that appear everywhere:\n",
    "\n",
    "### `Linear`\n",
    "A fully-connected layer that follows nn.Linear conventions:  \n",
    "`y = x @ Wᵀ + b`\n",
    "\n",
    "Important details:\n",
    "- Parameters should be registered as `nn.Parameter`\n",
    "- Store weight as (out_features, in_features) like nn.Linear.\n",
    "- The forward pass should support leading batch dimensions: `x` can be shape `(..., in_features)`\n",
    "\n",
    "### `Embedding`\n",
    "An embedding table maps integer ids to vectors:\n",
    "- input: token ids `idx` of shape `(...,)`\n",
    "- output: vectors of shape `(..., embedding_dim)`\n",
    "\n",
    "This is essentially a learnable lookup table.\n",
    "\n",
    "### `Dropout`\n",
    "Dropout randomly zeroes activations during training to reduce overfitting.\n",
    "Implementation details:\n",
    "- Only active in `model.train()` mode\n",
    "- In training: drop with probability `p` and scale the kept values by `1/(1-p)` so the expected value stays the same\n",
    "- In eval: return the input unchanged\n",
    "\n",
    "## Instructions\n",
    "- Do not use PyTorch reference seq for the parts you implement (e.g. don’t call nn.Linear inside your Linear).\n",
    "- You may use standard tensor ops that you learned before (matmul, sum, mean, rsqrt, indexing, etc.).\n",
    "- Use a parameter initialization method of your choice. We recommend something like Xavier-uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa9b6f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2933,  0.4604, -0.6958, -0.8306]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        weight = torch.zeros(size=(out_features, in_features))\n",
    "        torch.nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros([out_features]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., in_features)\n",
    "        return: (..., out_features)\n",
    "        \"\"\"\n",
    "        result = x@self.weight.T\n",
    "        if self.bias is not None:\n",
    "            result += self.bias\n",
    "        return result\n",
    "        \n",
    "Linear(5,4).forward(torch.ones(1,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2241e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0251,  0.5332,  0.6299],\n",
       "          [ 0.0380, -0.5404,  0.2357],\n",
       "          [ 0.0380, -0.5404,  0.2357]],\n",
       "\n",
       "         [[ 0.0251,  0.5332,  0.6299],\n",
       "          [ 0.0380, -0.5404,  0.2357],\n",
       "          [-0.5510, -0.1304, -0.0199]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0251,  0.5332,  0.6299],\n",
       "          [ 0.0380, -0.5404,  0.2357],\n",
       "          [ 0.0380, -0.5404,  0.2357]],\n",
       "\n",
       "         [[ 0.0251,  0.5332,  0.6299],\n",
       "          [ 0.0380, -0.5404,  0.2357],\n",
       "          [-0.5510, -0.1304, -0.0199]]]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(num_embeddings, embedding_dim))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (...,) int64\n",
    "        return: (..., embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.weight[idx]\n",
    "\n",
    "Embedding(10,3).forward(torch.tensor([[[0,1,1],[0,1,2]],[[0,1,1],[0,1,2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3390686f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.4286, 0.0000, 1.4286, 1.4286, 0.0000],\n",
       "         [1.4286, 0.0000, 1.4286, 0.0000, 0.0000],\n",
       "         [1.4286, 1.4286, 1.4286, 1.4286, 1.4286],\n",
       "         [0.0000, 1.4286, 1.4286, 1.4286, 1.4286],\n",
       "         [0.0000, 1.4286, 1.4286, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        In train mode: drop with prob p and scale by 1/(1-p).\n",
    "        In eval mode: return x unchanged.\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return x\n",
    "        prob = torch.zeros_like(x)\n",
    "        prob.uniform_(0,1)\n",
    "        return x.where(prob > self.p, 0) * (1/(1-self.p))\n",
    "\n",
    "dropout = Dropout(0.3)\n",
    "dropout.train()\n",
    "dropout.forward(torch.ones(1,5,5))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef77371",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization layers help stabilize training by controlling activation statistics.\n",
    "\n",
    "### LayerNorm\n",
    "LayerNorm normalizes each example across its **feature dimension** (the last dimension):\n",
    "\n",
    "- compute mean and variance over the last dimension\n",
    "- normalize: `(x - mean) / sqrt(var + eps)`\n",
    "- apply learnable per-feature scale and shift (`weight`, `bias`)\n",
    "\n",
    "**In this exercise, assume `elementwise_affine=True` (always include `weight` and `bias`).**  \n",
    "`weight` and `bias` each have shape `(D,)`.\n",
    "\n",
    "LayerNorm is widely used in transformers because it does not depend on batch statistics.\n",
    "\n",
    "### RMSNorm\n",
    "RMSNorm is similar to LayerNorm but normalizes using only the root-mean-square:\n",
    "- `x / sqrt(mean(x^2) + eps)` over the last dimension\n",
    "- usually includes a learnable scale (`weight`)\n",
    "- no mean subtraction\n",
    "\n",
    "RMSNorm is popular in modern LLMs because it's faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeaceef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.1970e-02,  8.1439e-01, -6.0850e-01,  1.2049e+00, -1.4299e+00,\n",
       "         -1.8404e+00,  8.6553e-01,  1.1368e+00, -4.3188e-02, -1.8165e-01],\n",
       "        [ 8.4333e-01, -1.7542e+00,  1.3156e+00, -1.2317e+00, -4.4581e-01,\n",
       "          2.3479e-01,  2.0232e-01,  1.5524e+00, -3.8874e-01, -3.2797e-01],\n",
       "        [-3.6453e-01,  4.0711e-01, -2.1992e-01,  6.6644e-01,  1.1085e+00,\n",
       "         -1.2439e-01, -6.3660e-01, -2.4007e+00,  2.4696e-01,  1.3171e+00],\n",
       "        [-7.6271e-01, -2.0349e-01, -5.7361e-01, -1.3986e+00,  1.2441e+00,\n",
       "          5.0966e-01, -1.3893e+00,  1.1372e+00,  1.4355e+00,  1.3458e-03],\n",
       "        [-8.6499e-01, -7.1066e-01, -1.6410e+00,  7.5933e-01, -8.1252e-01,\n",
       "          5.6496e-02,  6.2377e-01,  2.0034e+00, -5.2802e-02,  6.3908e-01],\n",
       "        [ 2.5750e-01, -9.5919e-01, -9.3506e-01,  7.1459e-01,  9.2600e-01,\n",
       "          1.1243e+00, -1.9616e+00, -5.1921e-01,  1.8902e-01,  1.1636e+00],\n",
       "        [-1.8026e+00, -5.0678e-02,  6.2770e-02,  3.0561e-02, -9.8517e-01,\n",
       "          3.2702e-01, -9.3140e-01,  7.0319e-01,  6.8995e-01,  1.9564e+00],\n",
       "        [-7.9763e-01,  2.5689e-03, -7.1383e-01,  1.7851e-01,  5.9398e-01,\n",
       "          6.1023e-01, -2.3285e-01,  2.1972e+00, -1.7925e+00, -4.5697e-02],\n",
       "        [ 1.7295e+00,  6.5954e-01, -9.5736e-02, -4.8365e-01, -2.8050e-01,\n",
       "         -2.3426e+00, -1.4473e-01, -1.0852e-01,  8.1866e-01,  2.4798e-01],\n",
       "        [-1.3287e+00, -3.9165e-01, -1.0051e+00, -8.3298e-01,  1.7504e+00,\n",
       "         -7.6673e-01, -1.9661e-01,  9.3062e-01,  6.6918e-01,  1.1716e+00]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, normalized_shape: int, eps: float = 1e-5, elementwise_affine: bool = True):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.weight = nn.Parameter(torch.ones([normalized_shape]))\n",
    "        self.bias = nn.Parameter(torch.zeros([normalized_shape]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize over the last dimension.\n",
    "        x: (..., D)\n",
    "        \"\"\"\n",
    "        layer_mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        layer_var = torch.var(x, dim=-1, correction=0, keepdim=True)\n",
    "        result = (x - layer_mean) / (layer_var + self.eps)**0.5\n",
    "        return result*self.weight + self.bias\n",
    "        \n",
    "LayerNorm(10).forward(torch.randn(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a2ac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1317, -0.2165,  1.2449,  0.3567,  1.2110,  0.4224, -0.4107, -1.1265,\n",
       "         -2.2438, -0.3764]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.weight = nn.Parameter(torch.ones([normalized_shape]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RMSNorm: x / sqrt(mean(x^2) + eps) * weight\n",
    "        over the last dimension.\n",
    "        \"\"\"\n",
    "        return (x / (torch.mean(x**2, dim=-1, keepdim=True)**0.5 + self.eps)) * self.weight\n",
    "RMSNorm(10).forward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f2b352",
   "metadata": {},
   "source": [
    "## MLPs and residual networks\n",
    "\n",
    "Now you’ll build larger networks by composing layers.\n",
    "\n",
    "### MLP\n",
    "An MLP is a stack of `depth` Linear layers with non-linear activations (use GELU) in between.\n",
    "In this exercise you’ll support:\n",
    "- configurable depth\n",
    "- a hidden dimension\n",
    "- optional LayerNorm between layers (a common stabilization trick)\n",
    "\n",
    "A key skill is building networks using `nn.ModuleList` / `nn.Sequential` while keeping shapes consistent.\n",
    "\n",
    "### Transformer-style FeedForward (FFN)\n",
    "A transformer block contains a position-wise feedforward network:\n",
    "- `D -> 4D -> D` (by default)\n",
    "- activation is typically **GELU**\n",
    "\n",
    "This is essentially an MLP applied independently at each token position.\n",
    "\n",
    "### Residual wrapper\n",
    "Residual connections are the simplest form of “skip connection”:\n",
    "- output is `x + fn(x)`\n",
    "\n",
    "They improve gradient flow and allow training deeper networks more reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22436c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (seq): Sequential(\n",
       "    (0): Linear()\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear()\n",
       "    (3): GELU(approximate='none')\n",
       "    (4): Linear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int,\n",
    "        use_layernorm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(Linear(in_dim, hidden_dim))\n",
    "        self.seq.append(nn.GELU())\n",
    "        for _ in range(depth):\n",
    "            if use_layernorm:\n",
    "                self.seq.append(LayerNorm(hidden_dim))\n",
    "            self.seq.append(Linear(hidden_dim, hidden_dim))\n",
    "            self.seq.append(nn.GELU())\n",
    "        if use_layernorm:\n",
    "            self.seq.append(LayerNorm(hidden_dim))\n",
    "        self.seq.append(Linear(hidden_dim, out_dim))        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.seq(x)\n",
    "\n",
    "MLP(10,10,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2169774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-style FFN: D -> 4D -> D (default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        # TODO: create two Linear layers and choose an activation (GELU)\n",
    "        self.linear1 = Linear(d_model, d_ff)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.gelu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80eef3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn: nn.Module):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        return x + self.fn(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b19d1a",
   "metadata": {},
   "source": [
    "## Classification problem\n",
    "\n",
    "In this section you’ll put everything together in a minimal MNIST classification experiment.\n",
    "\n",
    "You will:\n",
    "1) download and load the MNIST dataset\n",
    "2) implement cross-entropy from logits (stable, using log-softmax)\n",
    "3) build a simple MLP-based classifier (flatten MNIST images first)\n",
    "4) write a minimal training loop\n",
    "5) report train loss curve and final accuracy\n",
    "\n",
    "The goal here is not to reach state-of-the-art accuracy, but to understand the full pipeline:\n",
    "data → model → logits → loss → gradients → parameter update.\n",
    "\n",
    "### Model notes\n",
    "- We want you to combine the MLP we implemented above with the classification head we define below into one model \n",
    "\n",
    "### MNIST notes\n",
    "- MNIST images are `28×28` grayscale.\n",
    "- After `ToTensor()`, each image has shape `(1, 28, 28)` and values in `[0, 1]`.\n",
    "- For an MLP classifier, we flatten to a vector of length `784`.\n",
    "\n",
    "## Deliverables\n",
    "- Include a plot of your train loss curve in the video submission as well as a final accuracy. \n",
    "- **NOTE** Here we don't grade on model performance but we expect you to achieve at least 70% accuracy to confirm a correct model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00c05009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6306a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()  # -> float32 in [0,1], shape (1, 28, 28)\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds, batch_size)\n",
    "test_dl = DataLoader(test_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3781450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute mean cross-entropy loss from logits.\n",
    "\n",
    "    logits: (B, C)\n",
    "    targets: (B,) int64\n",
    "\n",
    "    Requirements:\n",
    "    - Use log-softmax for stability (do not use torch.nn.CrossEntropyLoss, we check this in the autograder).\n",
    "    \"\"\"\n",
    "    softmax = torch.exp(logits)/torch.sum(torch.exp(logits),dim=-1, keepdim=True)\n",
    "    mask = torch.nn.functional.one_hot(targets, num_classes=logits.size(-1)) == 1\n",
    "    return -torch.log(softmax).where(mask, 0).sum() / targets.size(0)\n",
    "\n",
    "cross_entropy_result = cross_entropy_from_logits(torch.ones(5,5), torch.ones([5],dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71242e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_in: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(d_in, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., d_in)\n",
    "        return: (..., num_classes) logits\n",
    "        \"\"\"\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3bd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(loader):\n",
    "    # TODO: You can use this function to evaluate your model accuracy.\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20555e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring epoch 0\n",
      "Test accuracy after batch 0: 10.94%\n",
      "Test accuracy after batch 20: 53.12%\n",
      "Test accuracy after batch 40: 65.62%\n",
      "Test accuracy after batch 60: 68.75%\n",
      "Test accuracy after batch 80: 75.00%\n",
      "Test accuracy after batch 100: 71.88%\n",
      "Test accuracy after batch 120: 79.69%\n",
      "Test accuracy after batch 140: 78.12%\n",
      "Test accuracy after batch 160: 79.69%\n",
      "Test accuracy after batch 180: 76.56%\n",
      "Test accuracy after batch 200: 81.25%\n",
      "Test accuracy after batch 220: 82.81%\n",
      "Test accuracy after batch 240: 79.69%\n",
      "Test accuracy after batch 260: 81.25%\n",
      "Test accuracy after batch 280: 82.81%\n",
      "Test accuracy after batch 300: 85.94%\n",
      "Test accuracy after batch 320: 85.94%\n",
      "Test accuracy after batch 340: 85.94%\n",
      "Test accuracy after batch 360: 82.81%\n",
      "Test accuracy after batch 380: 87.50%\n",
      "Test accuracy after batch 400: 85.94%\n",
      "Test accuracy after batch 420: 84.38%\n",
      "Test accuracy after batch 440: 89.06%\n",
      "Test accuracy after batch 460: 87.50%\n",
      "Test accuracy after batch 480: 85.94%\n",
      "Test accuracy after batch 500: 87.50%\n",
      "Test accuracy after batch 520: 87.50%\n",
      "Test accuracy after batch 540: 89.06%\n",
      "Test accuracy after batch 560: 89.06%\n",
      "Test accuracy after batch 580: 87.50%\n",
      "Test accuracy after batch 600: 92.19%\n",
      "Test accuracy after batch 620: 89.06%\n",
      "Test accuracy after batch 640: 89.06%\n",
      "Test accuracy after batch 660: 90.62%\n",
      "Test accuracy after batch 680: 87.50%\n",
      "Test accuracy after batch 700: 87.50%\n",
      "Test accuracy after batch 720: 89.06%\n",
      "Test accuracy after batch 740: 90.62%\n",
      "Test accuracy after batch 760: 89.06%\n",
      "Test accuracy after batch 780: 90.62%\n",
      "Test accuracy after batch 800: 89.06%\n",
      "Test accuracy after batch 820: 92.19%\n",
      "Test accuracy after batch 840: 92.19%\n",
      "Test accuracy after batch 860: 89.06%\n",
      "Test accuracy after batch 880: 89.06%\n",
      "Test accuracy after batch 900: 90.62%\n",
      "Test accuracy after batch 920: 90.62%\n",
      "Staring epoch 1\n",
      "Test accuracy after batch 0: 89.06%\n",
      "Test accuracy after batch 20: 89.06%\n",
      "Test accuracy after batch 40: 90.62%\n",
      "Test accuracy after batch 60: 87.50%\n",
      "Test accuracy after batch 80: 92.19%\n",
      "Test accuracy after batch 100: 90.62%\n",
      "Test accuracy after batch 120: 92.19%\n",
      "Test accuracy after batch 140: 90.62%\n",
      "Test accuracy after batch 160: 90.62%\n",
      "Test accuracy after batch 180: 85.94%\n",
      "Test accuracy after batch 200: 92.19%\n",
      "Test accuracy after batch 220: 92.19%\n",
      "Test accuracy after batch 240: 92.19%\n",
      "Test accuracy after batch 260: 90.62%\n",
      "Test accuracy after batch 280: 90.62%\n",
      "Test accuracy after batch 300: 92.19%\n",
      "Test accuracy after batch 320: 90.62%\n",
      "Test accuracy after batch 340: 89.06%\n",
      "Test accuracy after batch 360: 92.19%\n",
      "Test accuracy after batch 380: 92.19%\n",
      "Test accuracy after batch 400: 92.19%\n",
      "Test accuracy after batch 420: 92.19%\n",
      "Test accuracy after batch 440: 90.62%\n",
      "Test accuracy after batch 460: 90.62%\n",
      "Test accuracy after batch 480: 89.06%\n",
      "Test accuracy after batch 500: 89.06%\n",
      "Test accuracy after batch 520: 90.62%\n",
      "Test accuracy after batch 540: 89.06%\n",
      "Test accuracy after batch 560: 92.19%\n",
      "Test accuracy after batch 580: 90.62%\n",
      "Test accuracy after batch 600: 93.75%\n",
      "Test accuracy after batch 620: 92.19%\n",
      "Test accuracy after batch 640: 93.75%\n",
      "Test accuracy after batch 660: 92.19%\n",
      "Test accuracy after batch 680: 92.19%\n",
      "Test accuracy after batch 700: 92.19%\n",
      "Test accuracy after batch 720: 92.19%\n",
      "Test accuracy after batch 740: 92.19%\n",
      "Test accuracy after batch 760: 92.19%\n",
      "Test accuracy after batch 780: 92.19%\n",
      "Test accuracy after batch 800: 92.19%\n",
      "Test accuracy after batch 820: 92.19%\n",
      "Test accuracy after batch 840: 93.75%\n",
      "Test accuracy after batch 860: 92.19%\n",
      "Test accuracy after batch 880: 92.19%\n",
      "Test accuracy after batch 900: 90.62%\n",
      "Test accuracy after batch 920: 92.19%\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "def train_classifier(\n",
    "    model: nn.Module,\n",
    "    train_data_loader: DataLoader,\n",
    "    test_data_loader: DataLoader,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    seed: int = 0,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Minimal training loop for MNIST classification.\n",
    "\n",
    "    Steps:\n",
    "    - define optimizer\n",
    "    - for each epoch:\n",
    "        - sample minibatches\n",
    "        - forward -> cross-entropy -> backward -> optimizer step\n",
    "      - compute test accuracy at the end of each epoch\n",
    "    - return list of training losses (one per update step)\n",
    "\n",
    "    Requirements:\n",
    "    - call model.train() during training and model.eval() during evaluation\n",
    "    - do not use torch.nn.CrossEntropyLoss (use your cross_entropy_from_logits)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    model.to(device)\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    training_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    max_batches = 4000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Staring epoch {epoch}\")\n",
    "        for i, batch in enumerate(train_data_loader):\n",
    "            if i > max_batches:\n",
    "                break\n",
    "            model.train()\n",
    "            optim.zero_grad()\n",
    "            logits = model(batch[0].flatten(start_dim=1).to(device))\n",
    "            predicted_class = logits.argmax(dim=-1)\n",
    "            loss = cross_entropy_from_logits(logits, batch[1].to(device))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            training_losses.append(loss.item())\n",
    "\n",
    "            correct = total = 0\n",
    "            max_samples = 20\n",
    "            if i%max_samples == 0:\n",
    "                with torch.no_grad():\n",
    "                    for j, batch in enumerate(test_data_loader):\n",
    "                        if j >= max_samples:\n",
    "                            break\n",
    "                        model.eval()\n",
    "                        logits = model(batch[0].flatten(start_dim=1).to(device))\n",
    "                        predicted_class = logits.argmax(dim=-1)\n",
    "                        loss = cross_entropy_from_logits(logits, batch[1].to(device))\n",
    "                        test_losses.append(loss.item())\n",
    "                        correct = torch.where(predicted_class==batch[1].to(device), 1, 0).sum(dim=-1).item()\n",
    "                        total = predicted_class.size(-1)\n",
    "                        test_accuracy.append(correct*100/total)\n",
    "                    print(f\"Test accuracy after batch {i}: {correct*100/total:.2f}%\")\n",
    "    return training_losses\n",
    "\n",
    "model = MLP(in_dim=28*28, hidden_dim=28*14, out_dim=10, depth=3, use_layernorm=True)\n",
    "train_classifier(model, train_dl, test_dl, 1e-3, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
